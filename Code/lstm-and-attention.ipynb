{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8358293,"sourceType":"datasetVersion","datasetId":4967011}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install contractions num2words","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:01:47.890554Z","iopub.execute_input":"2024-05-08T16:01:47.890902Z","iopub.status.idle":"2024-05-08T16:02:02.295207Z","shell.execute_reply.started":"2024-05-08T16:01:47.890874Z","shell.execute_reply":"2024-05-08T16:02:02.294106Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting num2words\n  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\nCollecting textsearch>=0.0.21 (from contractions)\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: docopt>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from num2words) (0.6.2)\nCollecting anyascii (from textsearch>=0.0.21->contractions)\n  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\nCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\nDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nDownloading num2words-0.5.13-py3-none-any.whl (143 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, num2words, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 num2words-0.5.13 pyahocorasick-2.1.0 textsearch-0.0.24\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom torch.nn.functional import normalize as l2_norm\n\nimport random\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data.metrics import bleu_score\nfrom pprint import pprint\n\ndevice = \"cuda:0\" if torch.cuda.is_available() and torch.cuda.device_count() > 1 else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nin_dir = '/kaggle/input'\nout_dir = '/kaggle/working'\ntemp_dir = '/kaggle/temp'\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-08T16:02:02.297698Z","iopub.execute_input":"2024-05-08T16:02:02.298511Z","iopub.status.idle":"2024-05-08T16:02:19.071161Z","shell.execute_reply.started":"2024-05-08T16:02:02.298473Z","shell.execute_reply":"2024-05-08T16:02:19.070212Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-05-08 16:02:08.955054: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 16:02:08.955151: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 16:02:09.100514: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/combined-lince-and-cmu-dog/test_combined.csv\n/kaggle/input/combined-lince-and-cmu-dog/val_combined.csv\n/kaggle/input/combined-lince-and-cmu-dog/train_combined.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"data = {}\ntrain_data, dev_data, test_data = pd.read_csv(in_dir + '/combined-lince-and-cmu-dog/train_combined.csv'), pd.read_csv(in_dir + '/combined-lince-and-cmu-dog/val_combined.csv'), pd.read_csv(in_dir + '/combined-lince-and-cmu-dog/test_combined.csv')\ndata['X_train'], data['y_train'] = train_data['eng'].values, train_data['hing'].values\ndata['X_dev'], data['y_dev'] = dev_data['eng'].values, dev_data['hing'].values\ndata['X_test'], data['y_test'] = test_data['eng'].values, test_data['hing'].values","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:02:19.072482Z","iopub.execute_input":"2024-05-08T16:02:19.073220Z","iopub.status.idle":"2024-05-08T16:02:19.167394Z","shell.execute_reply.started":"2024-05-08T16:02:19.073184Z","shell.execute_reply":"2024-05-08T16:02:19.166462Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import string\nimport contractions\nfrom num2words import num2words\n\nimport nltk\n# nltk.download('punkt')\n\nfrom nltk.tokenize import word_tokenize\n\nfrom torchtext.vocab import build_vocab_from_iterator, Vocab\n\nSOS = '<s>'\nEOS = '</s>'\nUNK = '<UNK>'\nPAD = '<PAD>'\n\ndef convert_num2words(sent):\n    words = sent.split()\n    result_words = []\n    for word in words:\n        if word.isdigit():\n            result_words.append(num2words(int(word)))\n        else:\n            result_words.append(word)\n    return ' '.join(result_words)\n\ndef remove_punctuation(sent):\n    translator = str.maketrans('', '', string.punctuation)\n    return sent.translate(translator)\n\ndef pre_process(sent):\n    sent = contractions.fix(sent)\n    sent = sent.lower()\n    sent = convert_num2words(sent)\n    sent = remove_punctuation(sent)\n    sent = word_tokenize(sent)\n    return sent\n\nl = 1\ndata['train_tokens'] = [pre_process(i) for i in data['X_train'][ :: l]], [pre_process(i) for i in data['y_train'][ :: l]]\ndata['dev_tokens'] = [pre_process(i) for i in data['X_dev'][ :: l]], [pre_process(i) for i in data['y_dev'][ :: l]]\ndata['test_tokens'] = [pre_process(i) for i in data['X_test'][ :: l]], [pre_process(i) for i in data['y_test'][ :: l]]\n\neng_vocab = build_vocab_from_iterator(data['train_tokens'][0], specials=[UNK, PAD, SOS, EOS], min_freq=3)\neng_vocab.set_default_index(eng_vocab[UNK])\n\nhing_vocab = build_vocab_from_iterator(data['train_tokens'][1], specials=[UNK, PAD, SOS, EOS], min_freq=3)\nhing_vocab.set_default_index(hing_vocab[UNK])\n\ntorch.save(eng_vocab, out_dir + '/eng_vocab.pt')\ntorch.save(hing_vocab, out_dir + '/hing_vocab.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:02:19.168489Z","iopub.execute_input":"2024-05-08T16:02:19.168783Z","iopub.status.idle":"2024-05-08T16:02:26.324886Z","shell.execute_reply.started":"2024-05-08T16:02:19.168759Z","shell.execute_reply":"2024-05-08T16:02:26.324079Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def pad_collate(batch):\n  (xx, yy) = zip(*batch)\n  \n  xx = [torch.tensor([eng_vocab[SOS]] + [eng_vocab[token] for token in x] + [eng_vocab[EOS]]) for x in xx]\n  yy = [torch.tensor([hing_vocab[SOS]] + [hing_vocab[token] for token in y] + [hing_vocab[EOS]]) for y in yy]\n\n  x_pad = pad_sequence(xx, padding_value=eng_vocab[PAD])\n  y_pad = pad_sequence(yy, padding_value=hing_vocab[PAD])\n\n  return x_pad.to(device), y_pad.to(device)\n  \n\ntrain_data = list(zip(data['train_tokens'][0], data['train_tokens'][1]))\ndev_data = list(zip(data['dev_tokens'][0], data['dev_tokens'][1]))\ntest_data = list(zip(data['test_tokens'][0], data['test_tokens'][1]))\n\nbatch_size = 32\n\nloaders = {\n      'train': DataLoader(train_data, batch_size=batch_size, collate_fn=pad_collate, shuffle=False),\n      'dev': DataLoader(dev_data, batch_size=batch_size, collate_fn=pad_collate, shuffle=True),\n      'test': DataLoader(test_data, batch_size=batch_size, collate_fn=pad_collate, shuffle=True),\n    }","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:02:26.327181Z","iopub.execute_input":"2024-05-08T16:02:26.327490Z","iopub.status.idle":"2024-05-08T16:02:26.345093Z","shell.execute_reply.started":"2024-05-08T16:02:26.327464Z","shell.execute_reply":"2024-05-08T16:02:26.344350Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super(BahdanauAttention, self).__init__()\n        self.key_layer = nn.Linear(hidden_size, hidden_size)\n        self.query_layer = nn.Linear(hidden_size, hidden_size)\n        self.energy_layer = nn.Linear(hidden_size, 1)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden: [batch size, hidden size], encoder_outputs: [seq len, batch size, hidden size]\n        hidden = hidden.permute(0, 1, 2)\n        hidden = hidden.transpose(0, 1)\n        encoder_outputs = encoder_outputs.transpose(0, 1)\n#         print(f\"Query Shape: {hidden.size()}\")\n#         print(f\"Encoder Outputs Shape: {encoder_outputs.size()}\")\n        query_layer = self.query_layer(hidden)\n#         print(f\"Query Layer Shape: {query_layer.size()}\")\n        key_layer = self.key_layer(encoder_outputs)\n#         print(f\"Key Layer Shape: {key_layer.size()}\")\n        energy = self.energy_layer(torch.tanh(query_layer + key_layer))\n#         print(f\"Scores Shape: {energy.size()}\")\n        attention = energy.squeeze(2).unsqueeze(1)\n#         print(f\"Scores Shape: {energy.size()}\")\n\n        return self.softmax(attention)\n\nclass LuongAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super(LuongAttention, self).__init__()\n        self.softmax = nn.Softmax(dim=0)\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden: [batch size, hidden size], encoder_outputs: [seq len, batch size, hidden size]\n        attention = torch.bmm(encoder_outputs.transpose(0, 1), hidden.unsqueeze(2)).squeeze(2)\n        return self.softmax(attention.transpose(0, 1))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:02:26.346411Z","iopub.execute_input":"2024-05-08T16:02:26.346728Z","iopub.status.idle":"2024-05-08T16:02:26.372435Z","shell.execute_reply.started":"2024-05-08T16:02:26.346703Z","shell.execute_reply":"2024-05-08T16:02:26.371588Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class EncoderLSTM(nn.Module):\n  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n    super(EncoderLSTM, self).__init__()\n\n    # Size of the one hot vectors that will be the input to the encoder\n    #self.input_size = input_size\n\n    # Output size of the word embedding NN\n    #self.embedding_size = embedding_size\n\n    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n    self.hidden_size = hidden_size\n\n    # Number of layers in the lstm\n    self.num_layers = num_layers\n\n    # Regularization parameter\n    self.dropout = nn.Dropout(p)\n    self.tag = True\n\n    # Shape --------------------> (5376, 300) [input size, embedding dims]\n    self.embedding = nn.Embedding(input_size, embedding_size)\n    \n    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n\n  # Shape of x (26, 32) [Sequence_length, batch_size]\n  def forward(self, x):\n\n    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n    embedding = self.dropout(self.embedding(x))\n    \n    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n\n    return outputs, hidden_state, cell_state","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:05:20.188876Z","iopub.execute_input":"2024-05-08T16:05:20.189602Z","iopub.status.idle":"2024-05-08T16:05:20.208284Z","shell.execute_reply.started":"2024-05-08T16:05:20.189566Z","shell.execute_reply":"2024-05-08T16:05:20.207355Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"EncoderLSTM(\n  (dropout): Dropout(p=0.5, inplace=False)\n  (embedding): Embedding(3801, 128)\n  (LSTM): LSTM(128, 128, dropout=0.5)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"class DecoderLSTM(nn.Module):\n  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size, attn=False):\n    super(DecoderLSTM, self).__init__()\n\n    # Size of the one hot vectors that will be the input to the encoder\n    #self.input_size = input_size\n\n    # Output size of the word embedding NN\n    #self.embedding_size = embedding_size\n\n    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n    self.hidden_size = hidden_size\n\n    # Number of layers in the lstm\n    self.num_layers = num_layers\n\n    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n    self.output_size = output_size\n\n    # Regularization parameter\n    self.dropout = nn.Dropout(p)\n\n    # Shape --------------------> (5376, 300) [input size, embedding dims]\n    self.embedding = nn.Embedding(input_size, embedding_size)\n\n    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n    if attn:\n        embedding_size = embedding_size + hidden_size\n    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n\n    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n    self.fc = nn.Linear(hidden_size, output_size)\n    \n    self.attn = attn\n    if attn:\n        self.attention = BahdanauAttention(hidden_size)\n\n  # Shape of x (32) [batch_size]\n  def forward(self, x, hidden_state, cell_state, encoder_outputs):\n\n    # Shape of x (1, 32) [1, batch_size]\n    x = x.unsqueeze(0)\n\n    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n    lstm_input = self.dropout(self.embedding(x))\n#     print(f\"Embeddings Shape: {embedding.size()}\")\n#     print(f\"Hidden Shape: {hidden_state.size()}\")\n    if self.attn:\n        attention_weights = self.attention(hidden_state, encoder_outputs)\n    #     print(f\"Weights Shape: {attention_weights.size()}\")\n        context = torch.bmm(attention_weights, encoder_outputs.transpose(0, 1))\n        context = context.transpose(0, 1)\n    #     print(f\"Context Shape: {context.size()}\")\n        lstm_input = torch.cat((lstm_input, context), dim=2)\n#     print(f\"Input LSTM Shape: {lstm_input.size()}\")\n    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n    outputs, (hidden_state, cell_state) = self.LSTM(lstm_input, (hidden_state, cell_state))\n\n    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n    predictions = self.fc(outputs)\n\n    # Shape --> predictions (32, 4556) [batch_size , output_size]\n    predictions = predictions.squeeze(0)\n\n    return predictions, hidden_state, cell_state","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:05:20.318430Z","iopub.execute_input":"2024-05-08T16:05:20.319286Z","iopub.status.idle":"2024-05-08T16:05:20.347688Z","shell.execute_reply.started":"2024-05-08T16:05:20.319254Z","shell.execute_reply":"2024-05-08T16:05:20.346816Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"DecoderLSTM(\n  (dropout): Dropout(p=0.5, inplace=False)\n  (embedding): Embedding(4776, 128)\n  (LSTM): LSTM(128, 128, dropout=0.5)\n  (fc): Linear(in_features=128, out_features=4776, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n    super(Seq2Seq, self).__init__()\n    self.Encoder_LSTM = Encoder_LSTM\n    self.Decoder_LSTM = Decoder_LSTM\n\n  def forward(self, source, target, tfr=0.5):\n    # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n    batch_size = source.shape[1]\n\n    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n    target_len = target.shape[0]\n    target_vocab_size = len(hing_vocab)\n    \n    # Shape --> outputs (14, 32, 5766) \n    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n\n    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n    encoder_outputs, hidden_state, cell_state = self.Encoder_LSTM(source)\n\n    # Shape of x (32 elements)\n    x = target[0] # Trigger token <SOS>\n\n    for i in range(1, target_len):\n      # Shape --> output (32, 5766) \n      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state, encoder_outputs)\n      outputs[i] = output\n      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n\n    # Shape --> outputs (14, 32, 5766) \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2024-05-08T16:05:20.349816Z","iopub.execute_input":"2024-05-08T16:05:20.350243Z","iopub.status.idle":"2024-05-08T16:05:20.358244Z","shell.execute_reply.started":"2024-05-08T16:05:20.350212Z","shell.execute_reply":"2024-05-08T16:05:20.357366Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def translate_sentence(model, sentence, eng_vocab, hing_vocab, device, max_length=50):\n    tokens = pre_process(sentence) if type(sentence) == str else sentence.copy()\n    \n    tokens.insert(0, SOS)\n    tokens.append(EOS)\n    text_to_indices = [eng_vocab[token] for token in tokens]\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        encoder_outputs, hidden, cell = model.Encoder_LSTM(sentence_tensor)\n\n    outputs = [hing_vocab[SOS]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell, encoder_outputs)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == hing_vocab[EOS]:\n            break\n\n    translated_sentence = [hing_vocab.get_itos()[idx] for idx in outputs]\n    return translated_sentence[1:]\n\nweights = [(1, 0, 0, 0), \n           (0.5, 0.5), \n           (0.33, 0.33, 0.33, 0), \n           (0.25, 0.25, 0.25, 0.25),\n           (0.2, 0.2, 0.2, 0.2, 0.2),\n           (0.16, 0.16, 0.16, 0.16, 0.16, 0.16),\n           (0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14),\n           (0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125)]\n\ndef bleu_score(refs, candidates):\n  score = [0] * len(weights)\n  for i in range(len(refs)):\n    for j, w in enumerate(weights):\n      score[j] += sentence_bleu([refs[i]], candidates[i], weights=w)\n\n  for i in range(len(score)):\n    score[i] = score[i] / len(refs)\n    score[i] = round(score[i], 6)\n  \n  return sum(score) / len(score)\n\ndef bleu(data, model, eng_vocab, hing_vocab, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = example[0]\n        trg = example[1]\n\n        prediction = translate_sentence(model, src, eng_vocab, hing_vocab, device)\n        prediction = prediction[:-1]  # remove <eos> token\n        \n        targets.append(trg)\n        outputs.append(prediction)\n#         print(f\"English: {' '.join(src)}\")\n#         print(f\"Hinglish: {' '.join(trg)}\")\n#         print(f\"Translated: {' '.join(outputs[-1])}\")\n#         print('\\n')\n\n    return bleu_score(outputs, targets)\n\ndef checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n    print('saving')\n    print()\n    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n    torch.save(state, f'{out_dir}/checkpoint-LSTM')\n    torch.save(model.state_dict(),f'{out_dir}/checkpoint-LSTM-SD')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:12:29.705609Z","iopub.execute_input":"2024-05-08T17:12:29.706027Z","iopub.status.idle":"2024-05-08T17:12:29.724412Z","shell.execute_reply.started":"2024-05-08T17:12:29.705997Z","shell.execute_reply":"2024-05-08T17:12:29.723102Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def train(lr=0.001, dropout=0.5, hidden_size=128, embedding_size=128, attn=False):\n    # Hyperparameters\n    num_epochs = 15\n    input_size_encoder = len(eng_vocab)\n    encoder_embedding_size = embedding_size\n    hidden_size = hidden_size\n    num_layers = 1\n    encoder_dropout = dropout\n\n    encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n                               hidden_size, num_layers, encoder_dropout).to(device)\n    \n    input_size_decoder = len(hing_vocab)\n    decoder_embedding_size = embedding_size\n    hidden_size = hidden_size\n    decoder_dropout = dropout\n    output_size = len(hing_vocab)\n    attn = True\n    decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n                           hidden_size, num_layers, decoder_dropout, output_size, attn).to(device)\n    \n    learning_rate = lr\n    writer = SummaryWriter(f\"{out_dir}/runs/lstm_loss_plot\")\n    step = 0\n\n    model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    pad_idx = hing_vocab[PAD]\n    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n    best_loss = 999999\n    best_epoch = -1\n    sentence = \"The music in movie is very catchy and enjoyable, so I agree.\"\n    ts  = []\n\n    for epoch in range(num_epochs):\n      print(\"Epoch - {} / {}\".format(epoch + 1, num_epochs))\n      model.eval()\n      translated_sentence = translate_sentence(model, sentence, eng_vocab, hing_vocab, device, max_length=50)\n      print(f\"Translated example sentence: \\n {translated_sentence}\")\n      ts.append(translated_sentence)\n\n      model.train()\n      epoch_loss = 0.0\n      for batch_idx, (input, target) in enumerate(loaders['train']):\n        # Pass the input and target for model's forward method\n        output = model(input, target)\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        # Clear the accumulating gradients\n        optimizer.zero_grad()\n\n        # Calculate the loss value for every epoch\n        loss = criterion(output, target)\n\n        # Calculate the gradients for weights & biases using back-propagation\n        loss.backward()\n\n        # Clip the gradient value is it exceeds > 1\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Update the weights values using the gradients we calculated using bp \n        optimizer.step()\n        step += 1\n        epoch_loss += loss.item()\n        writer.add_scalar(\"Training loss\", loss, global_step=step)    \n\n      if epoch_loss < best_loss:\n        best_loss = epoch_loss\n        best_epoch = epoch\n        checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n        if ((epoch - best_epoch) >= 10):\n          print(\"no improvement in 10 epochs, break\")\n          break\n      print(\"Epoch_Loss - {}\".format(epoch_loss / len(loaders['train'])))\n      print()\n\n    print(epoch_loss / len(loaders['train']))\n\n    score = bleu(dev_data, model, eng_vocab, hing_vocab, device)\n#     details = {\n#         'learning_rate': lr,\n#         'dropout': dropout,\n#         'hidden_size': hidden_size,\n#         'embedding_size': embedding_size,\n#         'attention': attn,\n#         'bleu_score': round(score, 4)\n#     }\n#     scores.append(details)\n    print(f\"Bleu score {score * 100:.2f}\")\n    print(\"============================================================\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-08T22:50:10.349732Z","iopub.execute_input":"2024-05-08T22:50:10.350675Z","iopub.status.idle":"2024-05-08T22:50:10.368047Z","shell.execute_reply.started":"2024-05-08T22:50:10.350636Z","shell.execute_reply":"2024-05-08T22:50:10.367226Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"params = {\n    'attn': [False, True],\n    'lr': [0.001, 0.01, 0.1],\n    'dropout': [0, 0.5, 1],\n    'hidd_size': [64, 128, 256],\n    'embed_size': [64, 128, 256],\n}\n\n# scores = []\nfor attn in params['attn']:\n    for lr in params['lr']:\n        for drp in params['dropout']:\n            for hidd in params['hidd_size']:\n                    train(lr, drp, hidd, hidd, attn)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T21:43:19.207045Z","iopub.execute_input":"2024-05-08T21:43:19.207823Z","iopub.status.idle":"2024-05-08T22:45:10.461405Z","shell.execute_reply.started":"2024-05-08T21:43:19.207791Z","shell.execute_reply":"2024-05-08T22:45:10.459441Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch - 1 / 15\nTranslated example sentence: \n ['regarding', 'dilata', 'just', 'effect', 'determine', 'liam', 'people', 'upayog', 'mujhey', 'piece', 'common', 'mujhey', 'piece', 'life', 'life', 'gudamainhil', 'kisko', 'sangeetamay', 'sally', 'lagu', 'early', 'entertainment', 'chahoge', 'unn', 'sally', 'lagu', 'bahot', 'common', '5810', 'haiha', 'dusra', 'chicago', 'ratings', 'ship', 'subh', 'nikli', 'intrinsic', 'intrinsic', 'gyi', 'entertainment', 'namaskar', 'namaskar', 'bht', 'nirdeshak', 'nirdeshak', 'nirdeshak', 'ata', 'unexpected', 'rehna', 'jisake']\nsaving\n\nEpoch_Loss - 6.303222230502537\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['kya', '</s>']\nsaving\n\nEpoch_Loss - 5.992057704736316\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.839967604667422\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '<UNK>', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.748559134347098\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.682741496298048\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.621374402727399\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.56821072101593\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.5112007175173074\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.452935346535274\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.4130077589125865\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.353845998408302\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.309990935855442\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.266155771792881\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.220932327565693\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.171913936970726\n\n5.171913936970726\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Bleu score 27.31\n============================================================\nEpoch - 1 / 15\nTranslated example sentence: \n ['badhiya', 'trigger', 'cute', 'a', 'charles', 'experience', 'secretary', 'percent', 'color', 'george', 'omg', 'imdb', 'baltimore', 'baltimore', 'disqualify', 'obviously', 'george', 'kahin', 'anyatha', 'sweet', 'carl', 'jaaree', 'any', 'kiddy', 'historical', 'aisa', 'fear', 'koee', 'hole', 'gyaat', 'island', 'next', 'space', 'gyaat', 'suppose', 'scientist', 'lines', 'lete', 'personally', 'george', 'george', 'kahin', 'anyatha', 'sweet', 'carl', 'jaaree', 'any', 'kiddy', 'historical', 'aisa']\nsaving\n\nEpoch_Loss - 6.212055610285865\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.810341250328791\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.678671456518627\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.583051467698718\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.5068368012942965\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.436334354536874\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['kya', 'aap', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', 'ki', 'hai', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.368958798665849\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['kya', 'tum', 'movie', 'ke', 'baare', 'mein', '<UNK>', 'ke', 'baare', 'mein', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.28236934022298\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['kya', 'aap', 'movie', 'ke', 'baare', 'mein', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.203269537479159\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['kya', 'aap', 'ne', '<UNK>', 'movie', 'ke', 'baare', 'mein', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.131389349225968\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['kya', 'aap', 'is', 'movie', 'ke', 'baare', 'mein', 'hai', 'aur', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.062175763504846\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['kya', 'movie', 'ke', 'liye', 'mein', '<UNK>', 'hai', 'aur', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 4.995367445169934\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['movie', 'ka', 'naam', '<UNK>', 'hein', 'hein', 'lekin', 'mein', 'though', '<UNK>', 'hein', '</s>']\nsaving\n\nEpoch_Loss - 4.913713476960621\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['movie', 'ka', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.8641990054221385\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['movie', 'ka', '<UNK>', '<UNK>', 'ke', 'baare', 'mein', 'hai', 'aur', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.810280320190248\n\n4.810280320190248\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 5-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Bleu score 28.96\n============================================================\nEpoch - 1 / 15\nTranslated example sentence: \n ['koi', 'vas', 'lad', 'karte', 'princes', 'walker', 'dawn', 'doge', 'dam', 'robbers', 'mahal', 'rakhte', 'dubara', 'lad', 'karte', 'karte', 'tippanee', 'bacche', 'bacche', 'intrinsically', 'karne', 'princes', 'aap', 'deya', 'celine', 'age', 'village', 'thriller', 'lots', 'cofounder', 'hainks', 'eh', 'chali', 'force', 'melt', 'min', 'budget', 'batati', 'cuteness', 'imitation', 'jack', 'mi', 'ache', 'fitness', 'founder', 'probation', 'avengers', 'liking', 'light', 'usme']\nsaving\n\nEpoch_Loss - 6.0459660367360195\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.65329650186357\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['mujhe', 'lagta', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.4712304671605425\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['mujhe', 'lagta', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.2837904918761485\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['ye', 'movie', 'movie', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.114488942282541\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['movie', 'ek', '<UNK>', 'movie', 'hai', 'ki', 'hai', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 4.984321517130685\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['movie', 'ke', 'baare', 'mein', 'hai', 'hai', 'hai', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.865133980436931\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['movie', 'movie', 'movie', 'movie', 'mein', 'though', '<UNK>', 'hein', 'hein', '</s>']\nsaving\n\nEpoch_Loss - 4.7436104790558895\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['movie', 'movie', 'movie', 'thousand', 'and', 'thirteen', 'mein', 'aaya', 'tha', '</s>']\nsaving\n\nEpoch_Loss - 4.657588800267567\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['movie', 'movie', 'movie', 'mein', 'though', 'hein', 'hein', 'hein', 'ki', 'movie', 'hein', '</s>']\nsaving\n\nEpoch_Loss - 4.5552957483700345\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['movie', 'movie', 'mein', 'though', 'mein', 'hein', 'hein', 'ki', 'yeh', 'movie', 'movie', 'hein', '</s>']\nsaving\n\nEpoch_Loss - 4.46339190337393\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['movie', 'movie', 'movie', 'mein', 'hai', 'ki', 'hai', 'ki', 'movie', 'hai', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 4.363690764658035\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['movie', 'movie', 'movie', 'thousand', 'and', 'thirteen', 'mein', 'american', 'movie', 'thi', '</s>']\nsaving\n\nEpoch_Loss - 4.3003205472514745\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['movie', 'movie', 'movie', 'mein', 'though', 'mein', 'hein', 'hein', 'jo', 'ki', 'movie', 'mein', '</s>']\nsaving\n\nEpoch_Loss - 4.221275445960817\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['movie', 'movie', 'movie', 'mein', 'hai', 'ki', 'movie', 'hai', 'ki', 'movie', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 4.114126348779315\n\n4.114126348779315\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 6-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Bleu score 34.24\n============================================================\nEpoch - 1 / 15\nTranslated example sentence: \n ['kehna', 'creatures', 'atyant', 'inspired', 'bhul', 'inspired', 'fight', 'weekend', 'meaningless', 'kay', 'singers', 'thought', 'easier', 'easier', 'than', 'government', 'studios', 'banaaya', 'fbi', 'uski', 'ne', 'mordern', 'konse', 'hawo', 'nineteen', 'karega', 'hein', 'parwaah', 'pyaara', 'dekhoge', 'eighteen', 'ratai', 'ratai', 'bar', 'bar', 'aapka', 'vahan', 'immortality', 'mani', 'vishwas', 'combat', 'yeahuske', 'police', 'kisakee', 'pasad', 'pasad', 'thi', 'pehle', 'explain', 'skill']\nsaving\n\nEpoch_Loss - 6.081113867343418\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['kya', '</s>']\nsaving\n\nEpoch_Loss - 5.718144431000664\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.554939590749287\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.437329527877626\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.369832974577707\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['haan', 'ye', 'movie', 'ke', 'baare', 'mein', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.302396867956434\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.248941785759396\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.185610948100923\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.160071275536976\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.130813625123766\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['haan', 'mujhe', 'lagta', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.076432399333469\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.034335351179516\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['mujhe', 'lagta', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.005677358025596\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['haan', 'movie', 'movie', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.961085488871922\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['mujhe', 'lagta', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.9398944850951905\n\n4.9398944850951905\nBleu score 17.04\n============================================================\nEpoch - 1 / 15\nTranslated example sentence: \n ['captive', 'maartha', 'sam', 'sparks', 'idris', 'avengers', 'rag', 'liveaction', 'liveaction', 'sure', 'sure', 'hall', 'avengers', 'aayi', 'minions', 'bat', 'jeff', 'jeff', 'aamataur', 'dreamer', 'neat', 'dream', 'dream', 'dream', 'sochte', 'review', 'unhappy', 'gya', 'chijo', 'site', 'jisamen', 'chalne', 'extreme', 'difficult', 'kaunsi', 'thay', 'wonderful', 'wonderful', 'yaar', 'aata', 'badguys', 'usi', 'kese', 'bakio', 'illegal', 'padh', 'student', 'discussion', 'skty', 'accomplishment']\nsaving\n\nEpoch_Loss - 6.04422256303212\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['haan', '</s>']\nsaving\n\nEpoch_Loss - 5.6232880637759255\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['haan', 'wo', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.42570020755132\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.309756717984638\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.211212506842991\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.156669992303091\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['haan', 'yeh', 'though', '<UNK>', 'hein', 'jo', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.087011244088885\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['haan', 'yeh', 'though', '<UNK>', 'hein', 'ki', '<UNK>', '<UNK>', 'hein', 'jo', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.042760387299553\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.989211348787187\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.945878844885599\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.918276550277831\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.88580104614061\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.878301722189737\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.86161740810152\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.818965399549121\n\n4.818965399549121\nBleu score 22.69\n============================================================\nEpoch - 1 / 15\nTranslated example sentence: \n ['alava', 'alava', 'russian', 'loothar', 'already', 'jeff', 'nfl', 'preservation', 'iska', 'karaya', 'lose', 'humesha', 'sameekshaen', 'general', 'users', 'samajhaane', 'designs', 'thandi', 'zootopia', 'deserve', 'dark', 'suspiciously', 'founder', 'theatre', 'two', 'philmon', 'period', 'rhe', 'present', 'rachel', 'heartbreaking', 'tumhen', 'aapase', 'gender', 'message', 'hopefully', 'bruce', 'khas', 'slip', 'lol', 'creepy', '48', 'trol', 'personalities', 'charles', 'mad', 'redheads', 'nfl', 'gya', 'count']\nsaving\n\nEpoch_Loss - 6.012704198322599\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.5992126937896485\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['haan', 'mujhe', 'lagta', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.433169824736459\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['haan', 'mujhe', 'lagata', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.323638621776823\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.250539080018089\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.202347702450222\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['haan', 'yeh', 'though', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.162880248966671\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.104563904187036\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['haan', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.0431298859535705\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['haan', 'mujhe', 'lagata', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 5.032948868615287\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['haan', 'mujhe', 'lagata', 'hai', 'ki', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.996169165486381\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.9527335398727\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['kya', 'tum', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.922284353820104\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['is', 'movie', 'ko', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.9146732574417475\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 4.873509195115831\n\n4.873509195115831\nBleu score 21.33\n============================================================\nEpoch - 1 / 15\nTranslated example sentence: \n ['bharosa', 'genuinely', 'gea', 'fancy', 'jita', 'mad', 'inspire', 'overcompensate', 'items', 'chalne', 'honge', 'actor', 'heartwarming', 'jisne', 'ship', 'actor', 'villain', 'kharab', 'ghatna', 'craft', 'normal', 'missile', 'jahaan', 'ladder', 'bohot', 'don', 'instincts', 'tak', 'wiig', 'kryptonite', 'add', 'mjhy', 'their', 'afford', 'yojana', 'america', 'yaqeen', 'bareme', 'prospect', 'industries', 'philmen', 'palo', 'superstar', 'dukhee', 'baar', 'roles', 'update', 'comments', 'america', 'skty']\nsaving\n\nEpoch_Loss - 6.314708654842679\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['kya', '</s>']\nsaving\n\nEpoch_Loss - 6.162006581586505\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 6.136964677818238\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nEpoch_Loss - 6.137266559260232\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 6.127995612129332\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nEpoch_Loss - 6.133149340039208\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['kya', '</s>']\nsaving\n\nEpoch_Loss - 6.121292918447464\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nEpoch_Loss - 6.134626502082462\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nEpoch_Loss - 6.122935763427189\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 6.117843300577194\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nEpoch_Loss - 6.120641624170636\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nEpoch_Loss - 6.120877321750399\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nEpoch_Loss - 6.119774721917652\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 6.117019624937148\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['kya', '<UNK>', '</s>']\nsaving\n\nEpoch_Loss - 6.1060522643346635\n\n6.1060522643346635\nBleu score 5.71\n============================================================\nEpoch - 1 / 15\nTranslated example sentence: \n ['tyldum', 'patrick', 'priceless', 'mast', 'astrid', 'sweet', 'steward', 'hasi', 'nahin', 'harry', 'break', 'pretentious', 'meet', 'joss', 'trust', 'lal', 'macaulay', 'samiksha', 'kaarravaee', 'rehana', 'develop', 'villans', 'bullock', 'lene', 'kelie', 'edgerton', 'marvle', 'protective', 'achi', 'match', 'trick', 'defeat', 'dating', 'thor', 'valuable', 'kyuki', 'darne', 'isse', 'won', 'aanand', 'hamara', 'ninetytwo', 'setback', 'important', 'anyatha', 'kisi', 'button', 'private', 'con', 'beete']\nsaving\n\nEpoch_Loss - 6.516870946165115\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['haan', '</s>']\nsaving\n\nEpoch_Loss - 6.3039499246884905\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.327697521164303\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.311422677267165\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.314375911440168\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.3146881811202515\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.318055978843144\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['kya', '</s>']\nsaving\n\nEpoch_Loss - 6.301599006804209\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['kya', '</s>']\nsaving\n\nEpoch_Loss - 6.296710241408575\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['kya', '</s>']\nsaving\n\nEpoch_Loss - 6.274336470497979\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['kya', '</s>']\nsaving\n\nEpoch_Loss - 6.264432023441981\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.268301985566578\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.295168541726612\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.279024654910678\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.275364198381939\n\n6.275364198381939\nBleu score 7.39\n============================================================\nEpoch - 1 / 15\nTranslated example sentence: \n ['complex', 'rokta', 'favorites', 'tippanee', 'spasht', 'behtar', 'russian', 'aurello', 'dreamy', 'fan', 'sirph', 'kaun', 'kaam', 'until', 'grade', 'changes', 'bhumika', 'uss', 'fighting', 'ruffallo', 'ana', 'waih', 'early', 'recall', 'movie', 'jeetna', 'superman', 'bade', 'blood', 'pakke', 'de', 'blood', 'beauty', 'argument', 'humanity', 'lee', 'nfl', 'theek', 'great', 'not', 'bawajood', 'doob', 'ladkiyoon', 'kyu', 'sameekshaon', 'sacchi', 'milata', 'milata', 'tearjerker', 'yis']\nsaving\n\nEpoch_Loss - 7.022992005423894\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['kya', '</s>']\nsaving\n\nEpoch_Loss - 6.704890800846948\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.787710538932255\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.779581787094237\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.779613018035889\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['</s>']\nEpoch_Loss - 6.801847988650913\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.774130303708334\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.815426778225672\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['hai', '</s>']\nEpoch_Loss - 6.834367087909153\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.810703281372312\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.906093551999047\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['</s>']\nEpoch_Loss - 6.966816613598476\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['</s>']\nEpoch_Loss - 6.88693423403634\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.884012079428112\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['kya', '</s>']\nEpoch_Loss - 6.856212527978988\n\n6.856212527978988\nBleu score 6.40\n============================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame(sorted(scores, key=lambda x: x['bleu_score']))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T23:20:13.752989Z","iopub.execute_input":"2024-05-08T23:20:13.753942Z","iopub.status.idle":"2024-05-08T23:20:13.775091Z","shell.execute_reply.started":"2024-05-08T23:20:13.753901Z","shell.execute_reply":"2024-05-08T23:20:13.774208Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"  learning_rate  dropout  hidden_size  embedding_size  attention  bleu_score\n          0.100      0.5          128             128       True    0.061230\n          0.100      0.5           64              64      False    0.068300\n          0.100      0.5          256             256      False    0.077900\n          0.100      0.5          128             128      False    0.079600\n          0.100      0.5           64              64       True    0.117270\n          0.100      0.5          256             256       True    0.131562\n          0.010      0.5          256             256      False    0.198800\n          0.010      0.5          128             128      False    0.213600\n          0.010      0.5          128             128       True    0.251508\n          0.010      0.5           64              64      False    0.270600\n          0.001      0.5           64              64      False    0.277900\n          0.001      0.5          128             128      False    0.300100\n          0.010      0.5          256             256       True    0.300541\n          0.010      0.5           64              64       True    0.307635\n          0.001      0.5           64              64       True    0.322172\n          0.001      0.5          128             128       True    0.347912\n          0.001      0.5          256             256      False    0.351900\n          0.001      0.5          256             256       True    0.401365","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>learning_rate</th>\n      <th>dropout</th>\n      <th>hidden_size</th>\n      <th>embedding_size</th>\n      <th>attention</th>\n      <th>bleu_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th></th>\n      <td>0.100</td>\n      <td>0.5</td>\n      <td>128</td>\n      <td>128</td>\n      <td>True</td>\n      <td>0.061230</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.100</td>\n      <td>0.5</td>\n      <td>64</td>\n      <td>64</td>\n      <td>False</td>\n      <td>0.068300</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.100</td>\n      <td>0.5</td>\n      <td>256</td>\n      <td>256</td>\n      <td>False</td>\n      <td>0.077900</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.100</td>\n      <td>0.5</td>\n      <td>128</td>\n      <td>128</td>\n      <td>False</td>\n      <td>0.079600</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.100</td>\n      <td>0.5</td>\n      <td>64</td>\n      <td>64</td>\n      <td>True</td>\n      <td>0.117270</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.100</td>\n      <td>0.5</td>\n      <td>256</td>\n      <td>256</td>\n      <td>True</td>\n      <td>0.131562</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.010</td>\n      <td>0.5</td>\n      <td>256</td>\n      <td>256</td>\n      <td>False</td>\n      <td>0.198800</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.010</td>\n      <td>0.5</td>\n      <td>128</td>\n      <td>128</td>\n      <td>False</td>\n      <td>0.213600</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.010</td>\n      <td>0.5</td>\n      <td>128</td>\n      <td>128</td>\n      <td>True</td>\n      <td>0.251508</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.010</td>\n      <td>0.5</td>\n      <td>64</td>\n      <td>64</td>\n      <td>False</td>\n      <td>0.270600</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.001</td>\n      <td>0.5</td>\n      <td>64</td>\n      <td>64</td>\n      <td>False</td>\n      <td>0.277900</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.001</td>\n      <td>0.5</td>\n      <td>128</td>\n      <td>128</td>\n      <td>False</td>\n      <td>0.300100</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.010</td>\n      <td>0.5</td>\n      <td>256</td>\n      <td>256</td>\n      <td>True</td>\n      <td>0.300541</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.010</td>\n      <td>0.5</td>\n      <td>64</td>\n      <td>64</td>\n      <td>True</td>\n      <td>0.307635</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.001</td>\n      <td>0.5</td>\n      <td>64</td>\n      <td>64</td>\n      <td>True</td>\n      <td>0.322172</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.001</td>\n      <td>0.5</td>\n      <td>128</td>\n      <td>128</td>\n      <td>True</td>\n      <td>0.347912</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.001</td>\n      <td>0.5</td>\n      <td>256</td>\n      <td>256</td>\n      <td>False</td>\n      <td>0.351900</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>0.001</td>\n      <td>0.5</td>\n      <td>256</td>\n      <td>256</td>\n      <td>True</td>\n      <td>0.401365</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"model = train(hidden_size=256, embedding_size=256, attn=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T22:51:08.281028Z","iopub.execute_input":"2024-05-08T22:51:08.281415Z","iopub.status.idle":"2024-05-08T23:03:22.430129Z","shell.execute_reply.started":"2024-05-08T22:51:08.281390Z","shell.execute_reply":"2024-05-08T23:03:22.429150Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":60,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch - 1 / 15\nTranslated example sentence: \n ['muej', 'sipahi', 'bahar', 'bahar', 'bahar', 'real', 'jitana', 'coulson', 'recommend', 'obadiah', 'milate', 'share', 'bhadiya', 'pareshaani', 'is', 'maanana', 'sipahi', 'shikaaree', 'stereotype', 'film', 'sentence', 'turning', 'vipareet', 'warn', 'list', 'gye', 'pichhalee', 'comments', 'thawo', 'ayega', 'ninety', 'gang', 'received', 'abide', 'hang', 'baras', 'underdog', 'ladka', 'latest', 'critic', 'phir', 'vitamin', 'kristen', 'lagegi', 'wwe', 'akela', 'peeche', 'misread', 'magazines', 'theater']\nsaving\n\nEpoch_Loss - 6.025582270962851\n\nEpoch - 2 / 15\nTranslated example sentence: \n ['mujhe', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 5.588074549796089\n\nEpoch - 3 / 15\nTranslated example sentence: \n ['movie', 'movie', 'ke', 'baare', 'mein', 'hai', 'ki', 'movie', 'ke', 'liye', '</s>']\nsaving\n\nEpoch_Loss - 5.267911211838798\n\nEpoch - 4 / 15\nTranslated example sentence: \n ['movie', 'movie', 'ka', '<UNK>', 'hai', 'hai', 'ki', '<UNK>', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 4.985471700392072\n\nEpoch - 5 / 15\nTranslated example sentence: \n ['movie', 'movie', 'ka', 'naam', 'hai', 'aur', '<UNK>', 'mein', 'bhi', 'mein', '</s>']\nsaving\n\nEpoch_Loss - 4.7340673142009315\n\nEpoch - 6 / 15\nTranslated example sentence: \n ['is', 'movie', 'ka', '<UNK>', 'aur', 'aur', 'aur', 'mein', 'bhi', 'mein', '</s>']\nsaving\n\nEpoch_Loss - 4.510571883311347\n\nEpoch - 7 / 15\nTranslated example sentence: \n ['movie', 'movie', 'two', 'thousand', 'and', 'hundred', 'and', 'twenty', 'me', '<UNK>', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 4.297145872362076\n\nEpoch - 8 / 15\nTranslated example sentence: \n ['movie', 'movie', 'two', 'thousand', 'and', 'thirteen', 'me', 'hai', 'mujhe', 'mujhe', 'sahamat', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 4.102777879389506\n\nEpoch - 9 / 15\nTranslated example sentence: \n ['movie', 'movie', 'two', 'thousand', 'and', 'thirteen', 'mein', 'hai', 'mujhe', 'sahamat', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 3.947558431398301\n\nEpoch - 10 / 15\nTranslated example sentence: \n ['is', 'movie', 'two', 'thousand', 'and', 'hundred', 'and', 'twenty', 'me', 'sahamat', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 3.7693266135359567\n\nEpoch - 11 / 15\nTranslated example sentence: \n ['is', 'movie', 'mein', 'two', 'thousand', 'and', 'twenty', 'hai', 'hai', 'i', 'agree', 'karta', 'hu', '</s>']\nsaving\n\nEpoch_Loss - 3.6188246092152974\n\nEpoch - 12 / 15\nTranslated example sentence: \n ['is', 'movie', 'mein', 'bahut', 'funny', 'hai', 'hai', 'mujhe', 'am', 'sahamat', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 3.4894763172145873\n\nEpoch - 13 / 15\nTranslated example sentence: \n ['is', 'movie', 'two', 'thousand', 'and', 'and', 'and', 'twenty', 'mein', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 3.367224615481165\n\nEpoch - 14 / 15\nTranslated example sentence: \n ['movie', 'movie', 'mein', 'bahut', 'funny', 'hai', 'aur', 'bahut', 'hee', 'sahamat', 'hai', '</s>']\nsaving\n\nEpoch_Loss - 3.2384056500972265\n\nEpoch - 15 / 15\nTranslated example sentence: \n ['movie', 'movie', 'two', 'thousand', 'and', 'twenty', 'me', 'sahamat', 'hoon', 'lol', '</s>']\nsaving\n\nEpoch_Loss - 3.138610839134171\n\n3.138610839134171\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 6-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 5-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 7-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 8-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Bleu score 40.18\n============================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"sent = translate_sentence(model, 'Have you seen Inception before?', eng_vocab, hing_vocab, device, max_length=50)\n' '.join(sent[:-1])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T23:07:43.009444Z","iopub.execute_input":"2024-05-08T23:07:43.009803Z","iopub.status.idle":"2024-05-08T23:07:43.027694Z","shell.execute_reply.started":"2024-05-08T23:07:43.009777Z","shell.execute_reply":"2024-05-08T23:07:43.026800Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"'kya tumne pehle dekha dekhi hai'"},"metadata":{}}]}]}